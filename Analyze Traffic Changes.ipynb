{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Changes in Traffic\n",
    "\n",
    "### Initial Setup \n",
    "- This script uses the GSC API \n",
    "- If you don't have your GSC API Key, watch this video from Jean Chouinard first: https://www.youtube.com/watch?v=-uy4L4P1Ujs&t=4s\n",
    "- This code was built off June Tao Ching's GSC code and was modified to audit my content accordingly\n",
    "    - Source: https://towardsdatascience.com/access-google-search-console-data-on-your-site-by-using-python-3c8c8079d6f8\n",
    "\n",
    "### What does the code do?\n",
    "\n",
    "- The following code pulls Keyword and URL Level Data from GSC for 2 different date ranges \n",
    "- Then it joins the data together and subtracts clicks, avg. position and impressions at the URL and KW level\n",
    "- Next, we take a look at Keywords, subfolders and URLs that saw the largest drop in traffic\n",
    "- We will then run an Ngram analysis on keywords that saw the largest drop in traffic\n",
    "- Bonus: If you want to dive a little deeper, you can run a screaming frog crawl to get titles, H-1s and h-2s for urls with the largest traffic drops and then run an ngram analysis on the titles to find patterns amongst your traffic losers  (scroll to the bottom of my code for this analysis)\n",
    "\n",
    "\n",
    "\n",
    "### What is an Ngram Analysis?\n",
    "- \"An n-gram is a collection of n successive items in a text document that may include words, numbers, symbols, and punctuation.\" - https://www.mathworks.com/discovery/ngram.html\n",
    "\n",
    "### How can Ngrams be used for SEO?\n",
    "- You can use Ngrams to analyze keywords to find patterns patterns, topic cluster / internal linking ideas \n",
    "- Ngrams can used to analyzeyour page titles, urls, keywords, H-1s, anchor text (internal and external) to find specific patterns \n",
    "\n",
    "#### Other Examples:\n",
    "- Ngrams on GSC Keywords to find Topic Clusters / Internal Linking opportunities\n",
    "- Ngrams on competitor Keyword data / titles / H-1s to find other keywords / topics to build out\n",
    "- Ngrams on competitors titles (best by links (ahrefs report)) to determine which pages are generating the most backlinks\n",
    "- Ngrams on GSC Keywords / page titles to analyze the impact of an algorithm update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gsc_api:\n",
    "\n",
    "    def __init__(self,website,start_date,end_date):\n",
    "\n",
    "        self.website = website\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "\n",
    "    \n",
    "\n",
    "    def url_level_data(self):\n",
    "\n",
    "        SITE_URL = self.website\n",
    "\n",
    "        OAUTH_SCOPE = ('https://www.googleapis.com/auth/webmasters.readonly', 'https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "        # Redirect URI for installed apps\n",
    "        REDIRECT_URI = 'urn:ietf:wg:oauth:2.0:oob'\n",
    "        \n",
    "        \n",
    "        # You must edit gsc_credentials and pickled_credentials to include YOUR username\n",
    "        gsc_credentials = r'your credentials '\n",
    "        \n",
    "        # where your pickled credential will be stored\n",
    "\n",
    "        pickled_credentials = r'c:\\users\\your_username\\desktop\\pickled_credential'\n",
    "\n",
    "\n",
    "        try:\n",
    "            credentials = pickle.load(open(pickled_credentials  + \".pickle\", \"rb\"))\n",
    "        except (OSError, IOError) as e:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(gsc_credentials, scopes=OAUTH_SCOPE)\n",
    "            credentials = flow.run_console()\n",
    "            pickle.dump(credentials, open(pickled_credentials  + \".pickle\", \"wb\"))\n",
    "\n",
    "            # Connect to Search Console Service using the credentials \n",
    "        webmasters_service = build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "        maxRows = 25000\n",
    "        i = 0\n",
    "        output_rows = []\n",
    "        start_date = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        def date_range(start_date, end_date, delta=timedelta(days=1)):\n",
    "\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                yield current_date\n",
    "                current_date += delta\n",
    "        print('script start date:', start_date)\n",
    "\n",
    "        for date in date_range(start_date, end_date):\n",
    "            date = date.strftime(\"%Y-%m-%d\")\n",
    "            i = 0\n",
    "            while True:\n",
    "\n",
    "                request = {\n",
    "                    'startDate' : date,\n",
    "                    'endDate' : date,\n",
    "                    'dimensions' : [\"page\"],\n",
    "                    \"searchType\": \"Web\",\n",
    "                    'rowLimit' : maxRows,\n",
    "                    'startRow' : i * maxRows\n",
    "                }\n",
    "\n",
    "                response = webmasters_service.searchanalytics().query(siteUrl = SITE_URL, body=request).execute()\n",
    "                if response is None:\n",
    "                    break\n",
    "                if 'rows' not in response:\n",
    "                    break\n",
    "                else:\n",
    "                    for row in response['rows']:\n",
    "                        page = row['keys'][0]\n",
    "                        output_row = [page, row['clicks'], row['impressions'], row['position']]\n",
    "                        output_rows.append(output_row)\n",
    "                    i = i + 1\n",
    "        print('script end date:', end_date)\n",
    "\n",
    "        df = pd.DataFrame(output_rows, columns=['Address', 'URL Clicks', 'URL Impressions', 'URL Average Position'])\n",
    "        df = df.groupby(['Address']).agg({'URL Clicks':'sum','URL Impressions':'sum','URL Average Position':'mean'}).reset_index()\n",
    "        df['URL CTR'] = df['URL Clicks'] / df['URL Impressions'] \n",
    "        return df\n",
    "\n",
    "    def gsc_kw(self):\n",
    "\n",
    "        SITE_URL = self.website\n",
    "\n",
    "        OAUTH_SCOPE = ('https://www.googleapis.com/auth/webmasters.readonly', 'https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "        # Redirect URI for installed apps\n",
    "        REDIRECT_URI = 'urn:ietf:wg:oauth:2.0:oob'\n",
    "        \n",
    "        \n",
    "        # You must edit gsc_credentials and pickled_credentials to include YOUR username\n",
    "        gsc_credentials = r'your credentials '\n",
    "        \n",
    "        # where your pickled credential will be stored\n",
    "\n",
    "        pickled_credentials = r'c:\\users\\your_username\\desktop\\pickled_credential'\n",
    "        \n",
    "\n",
    "\n",
    "        try:\n",
    "            credentials = pickle.load(open(pickled_credentials  + \".pickle\", \"rb\"))\n",
    "        except (OSError, IOError) as e:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(gsc_credentials, scopes=OAUTH_SCOPE)\n",
    "            credentials = flow.run_console()\n",
    "            pickle.dump(credentials, open(pickled_credentials  + \".pickle\", \"wb\"))\n",
    "\n",
    "            # Connect to Search Console Service using the credentials \n",
    "        webmasters_service = build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "        maxRows = 25000\n",
    "        i = 0\n",
    "        output_rows = []\n",
    "        start_date = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        def date_range(start_date, end_date, delta=timedelta(days=1)):\n",
    "\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                yield current_date\n",
    "                current_date += delta\n",
    "        print('script start date:', start_date)\n",
    "\n",
    "        for date in date_range(start_date, end_date):\n",
    "            date = date.strftime(\"%Y-%m-%d\")\n",
    "            i = 0\n",
    "            while True:\n",
    "\n",
    "                request = {\n",
    "                    'startDate' : date,\n",
    "                    'endDate' : date,\n",
    "                    'dimensions' : [\"page\",'query'],\n",
    "                    \"searchType\": \"Web\",\n",
    "                    'rowLimit' : maxRows,\n",
    "                    'startRow' : i * maxRows\n",
    "                }\n",
    "\n",
    "                response = webmasters_service.searchanalytics().query(siteUrl = SITE_URL, body=request).execute()\n",
    "                if response is None:\n",
    "                    break\n",
    "                if 'rows' not in response:\n",
    "                    break\n",
    "                else:\n",
    "                    for row in response['rows']:\n",
    "                        page = row['keys'][0]\n",
    "                        keyword = row['keys'][1]\n",
    "                        output_row = [ page,keyword, row['clicks'], row['impressions'], row['ctr'], row['position']]\n",
    "                        output_rows.append(output_row)\n",
    "                    i = i + 1\n",
    "        print('script end date:', end_date)\n",
    "\n",
    "        df = pd.DataFrame(output_rows, columns=['Address','Main Keyword', 'KW Clicks', 'KW Impressions', 'KW CTR',  'KW Average Position'])\n",
    "        df = df.groupby(['Address','Main Keyword']).agg({'KW Clicks':'sum','KW Impressions':'sum','KW Average Position':'mean'}).reset_index()\n",
    "        df['KW CTR'] = df['KW Clicks'] / df['KW Impressions'] \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ngram class\n",
    "\n",
    "class n_gram:\n",
    "\n",
    "    def __init__(self,data, column):\n",
    "\n",
    "        self.data = data\n",
    "        self.column = column\n",
    "        \n",
    "    def generate_N_grams(self,text,ngram=1):\n",
    "      self.text = text\n",
    "      self.ngram = ngram\n",
    "      words=[word for word in text.split(\" \")]  \n",
    "      print(\"Sentence after removing stopwords:\",words)\n",
    "      temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "      ans=[' '.join(ngram) for ngram in temp]\n",
    "      return ans\n",
    "        \n",
    "    def n_gram_function(self , s):  \n",
    "        data = self.data\n",
    "        column = self.column\n",
    "        gram = defaultdict(int)\n",
    "        for text in data[column]:\n",
    "              for word in self.generate_N_grams(text,s):\n",
    "                gram[word]+=1\n",
    "        gram = pd.DataFrame(sorted(gram.items(),key=lambda x:x[1],reverse=True))\n",
    "        return gram\n",
    "    \n",
    "    \n",
    "    ## Returns Unigram \n",
    "    def unigram(self):\n",
    "        unigram = self.n_gram_function(1)\n",
    "        return unigram\n",
    "    \n",
    "    ## Returns Bigram \n",
    "    def bigram(self):\n",
    "        bigram = self.n_gram_function(2)\n",
    "        return bigram\n",
    "\n",
    "    ## Returns Trigram \n",
    "    def trigram(self):\n",
    "        trigram = self.n_gram_function(3)\n",
    "        return trigram\n",
    "\n",
    "    ## Returns Quadgram \n",
    "    def quadgram(self):\n",
    "        quadgram = self.n_gram_function(4)\n",
    "        return quadgram\n",
    "    \n",
    "    ## Returns fivegram \n",
    "    def quintgram(self):\n",
    "        quintgram = self.n_gram_function(5)\n",
    "        return quintgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code pulls data from GSC's API for the date ranges we want to compare\n",
    "\n",
    "current_date = gsc_api('https://yourwebsite.com/','2022-08-01','2022-10-31')\n",
    "prev_date = gsc_api('https://yourwebsite.com/','2022-05-01','2022-07-31')\n",
    "\n",
    "# We are requesting URL and KW level data here \n",
    "\n",
    "url_level_data_current = current_date.url_level_data()\n",
    "kw_level_data_current = current_date.gsc_kw()\n",
    "\n",
    "url_level_data_prev = prev_date.url_level_data()\n",
    "kw_level_data_prev = prev_date.gsc_kw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this code merges data from the 2 date ranges we requested. \n",
    "Then it does some basic subtraction to see differences in Clicks, Impressions and Average Position at the URL level\n",
    "as well as KW level changes '''\n",
    "\n",
    "\n",
    "url_level_data_prev = url_level_data_prev.rename(columns = {'URL Clicks':'Prev URL Clicks','URL Impressions':'Prev URL Impressions','URL Average Position':'Prev URL Average Position','URL CTR':'Prev URL CTR'})\n",
    "\n",
    "df = url_level_data_current.merge(url_level_data_prev, how = 'outer',  on = 'Address')\n",
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "# subtracting Clicks, Impressions and Avg. Positions at the  URL level \n",
    "\n",
    "df['URL Clicks Diff'] = df['URL Clicks'] - df['Prev URL Clicks']\n",
    "df['URL Impressions Diff'] = df['URL Impressions'] - df['Prev URL Impressions']\n",
    "df['URL Average Position Diff'] =  df['Prev URL Average Position'] - df['URL Average Position']\n",
    "traffic_drop = df[df['URL Clicks Diff'] <= -1]\n",
    "traffic_drop['Subfolder'] = traffic_drop['Address'].str.split('/').str[3]\n",
    "subfolder_drops = traffic_drop[['Subfolder','URL Clicks Diff','URL Impressions Diff','URL Average Position Diff']].groupby('Subfolder').agg(['sum','mean', 'count']).reset_index()\n",
    "df = df.fillna(0)\n",
    "\n",
    "df['URL Clicks Diff'] = df['URL Clicks'] - df['Prev URL Clicks']\n",
    "df['URL Impressions Diff'] = df['URL Impressions'] - df['Prev URL Impressions']\n",
    "df['URL Average Position Diff'] =  df['Prev URL Average Position'] - df['URL Average Position']\n",
    "\n",
    "traffic_drop = df[df['URL Clicks Diff'] <= -1]\n",
    "traffic_drop['Subfolder'] = traffic_drop['Address'].str.split('/').str[3]\n",
    "\n",
    "subfolder_drops = traffic_drop[['Subfolder','URL Clicks Diff','URL Impressions Diff','URL Average Position Diff']].groupby('Subfolder').agg(['sum','mean', 'count']).reset_index()\n",
    "\n",
    "subfolder_drops.columns = ['_'.join(col) for col in subfolder_drops.columns]\n",
    "\n",
    "subfolder_drops.sort_values(by = 'URL Clicks Diff_sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subfolder Changes\n",
    "- the below code will show you subfolders that had the biggest drop in traffic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder_drops.sort_values(by = 'URL Clicks Diff_sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Keywords that saw a dip in Clicks and Avg Position \n",
    "- Criteria may need to be adjusted depending on the size of your site! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge keyword data and subtract to find drops in clicks, impressions and avg. position\n",
    "\n",
    "kw_level_data_prev = kw_level_data_prev.rename(columns = {'KW Clicks':'Prev KW Clicks','KW Impressions':'Prev KW Impressions','KW Average Position':'Prev KW Average Position','KW CTR':'Prev KW CTR'})\n",
    "kw_data = kw_level_data_current.merge(kw_level_data_prev, how = 'outer', on =['Address','Main Keyword'])\n",
    "\n",
    "kw_data[kw_data.columns[2:]] = kw_data[kw_data.columns[2:]].fillna(0)\n",
    "\n",
    "kw_data['KW Clicks Diff']= kw_data['KW Clicks'] - kw_data['Prev KW Clicks']\n",
    "kw_data['KW Impressions Diff']= kw_data['KW Impressions'] - kw_data['Prev KW Impressions']\n",
    "kw_data['KW Average Position Diff']= kw_data['Prev KW Average Position'] - kw_data['KW Average Position']\n",
    "kw_data['KW CTR Diff']= kw_data['KW CTR'] - kw_data['Prev KW CTR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_drops = kw_data[(kw_data['KW Clicks Diff'] <= -1) & (kw_data['KW Average Position Diff'] <= -1)].sort_values(by = 'KW Clicks Diff').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return 50 Keywords that saw the biggest drop in traffic \n",
    "kw_drops.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an Ngram Analysis on your keyword data \n",
    "\n",
    "#### Questions to ask yourself:\n",
    "- Are there certain keywords that are dropping in ranks & clicks? (Review / Best / How to / What is , etc.) \n",
    "- Are there topics on your site that are dropping in rank and clicks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run an n_gram analysis \n",
    "ngram_kw_data =  n_gram(kw_drops, column = 'Main Keyword')\n",
    "\n",
    "# Bigram\n",
    "bigram_kw_data = ngram_kw_data.bigram()\n",
    "\n",
    "# Trigram\n",
    "trigram_kw_data = ngram_kw_data.trigram()\n",
    "\n",
    "# Quadgram\n",
    "quadgram_kw_data = ngram_kw_data.quadgram()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an Ngram Analysis on your URL Level data \n",
    "- The following code runs a Screaming Frog Crawl on your website to return Title, H-1 and H-2s for each URL\n",
    "- then merges it with the GSC URL level data we pulled from earlier \n",
    "- Finally, we can run an Ngram analysis on our Title, H-1 or H-2s\n",
    "\n",
    "#### Questions to ask yourself:\n",
    "- Are there certain URLs / Page titles that are dropping in ranks & clicks? (Review / Best / How to / What is , etc.) \n",
    "- Are certain topics on your site being affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sf_crawl:\n",
    "\n",
    "    def __init__(self,website, output_folder):\n",
    "\n",
    "        self.website = website\n",
    "        self.output_folder = output_folder\n",
    "        \n",
    "    def urls(self):\n",
    "        website = self.website\n",
    "        output_folder = self.output_folder\n",
    "        sf_command = os.system('cd \"C:\\Program Files (x86)\\Screaming Frog SEO Spider\" && ScreamingFrogSEOSpiderCli.exe --crawl {} --headless --output-folder {} --export-tabs \"Internal:All\"'\\\n",
    "            .format(website,output_folder))\n",
    "        df = pd.read_csv(output_folder + '\\internal_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl = sf_crawl('https://yourwebsite.com/',output_folder = r'c:\\users\\your_output_folder')\n",
    "\n",
    "sf_data = crawl.urls()\n",
    "\n",
    "# filters out non-indexable URLs\n",
    "\n",
    "sf_data = sf_data[sf_data['Indexability'] == 'Indexable'][['Address','Indexability','Title 1','H1-1','H2-1','H2-2']]\n",
    "\n",
    "traffic_drop = traffic_drop.merge(sf_data, how = 'left', on ='Address')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run an n_gram analysis on URLs that had the largest traffic drop \n",
    "ngram_url_data =  n_gram(traffic_drop, column = 'Title 1')\n",
    "\n",
    "# Bigram\n",
    "bigram_url_data = ngram_url_data.bigram()\n",
    "\n",
    "# Trigram\n",
    "trigram_url_data = ngram_url_data.trigram()\n",
    "\n",
    "# Quadgram\n",
    "quadgram_url_data = ngram_url_data.quadgram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
