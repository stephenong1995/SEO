{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical and Page Speed Audit\n",
    "\n",
    "### Initial Setup \n",
    "- This script runs Screaming Frog crawls via the command line\n",
    "- You need a paid subscription to Screaming Frog and a Page Speed API Key before proceeding \n",
    "    - API Key Link - https://developers.google.com/speed/docs/insights/v5/get-started\n",
    "- Read More about running Screaming Frog on the command line here:\n",
    "    - https://www.screamingfrog.co.uk/seo-spider/user-guide/general/#command-line\n",
    "\n",
    "### What does the code do?\n",
    "\n",
    "- The following code runs a screaming frog crawl via the command line. There are 2 methods in our class. The first method runs a standard technical SEO Audit and the second runs a page speed audit using the PSI API. \n",
    "- the code will also save an excel audit file to your specified output folder \n",
    "\n",
    "### How to prioritize my efforts?\n",
    "- As an SEO Professional, it will be your decision to interpret the data and prioritize your efforts accordingly. It can be hard to fix every single technical item, so you need to decide which items warrant a fix since every site is different.\n",
    "- In some cases, canonicals may be off on key pages and on others, your robots.txt file may be blocking the crawl to key subfolders\n",
    "- If you have key money pages that are really slow and have a terrible UX, you may want to improve the page speed of them\n",
    "\n",
    "### Technical SEO Audit\n",
    "- the code below will run Screaming Frog Crawls via the command line / prompt, then save all the technical issues to an excel file\n",
    "- Here are the issues we check for:\n",
    "    - Non-Indexable URLs (200s)\n",
    "    - Non-Self Referencing Canonicals \n",
    "    - Pages with Multiple H-1s\n",
    "    - Pages with Missing Headings (H-1s and H-2s)\n",
    "    - Structured Data Errors  \n",
    "    - Redirect Chains\n",
    "    - JavaScript Redirects\n",
    "    - URLs that Canonicalize to Non-Indexable Pages\n",
    "    - Meta Refreshes\n",
    "    - All Non-200 Status Code Outlinks (For any 301 redirects, we ran an additional crawl to get the final destination URLs) \n",
    "    \n",
    "    \n",
    "### Page Speed Audit\n",
    "- the code below will run a page speed audit via API Key (using Screaming Frog) via the command line / prompt, then save all the technical issues in an excel file\n",
    "- Here are the issues we check for:\n",
    "    - Page Speed Metrics at the URL Level (Including CWV)\n",
    "    - Render-blocking resources\n",
    "    - Assets that lengthen their cache policy \n",
    "    - Images that aren't properly sized\n",
    "    - Offscreen Images that need to be deferred  \n",
    "    - Images not in Next-Gen Format\n",
    "    - Minified JS, CSS\n",
    "    - Unused JS and CSS \n",
    "    - JS with Long Execution Time \n",
    "    - Elements avoiding layout shifts\n",
    "    - Image elemnts that don't have explicit height and Width \n",
    "    - Excessive Dom Size\n",
    "- Sf also has a summary tab that shows where we can improve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "class sf_crawl_audit:\n",
    "\n",
    "    def __init__(self,website, output_folder):\n",
    "\n",
    "        self.website = website\n",
    "        self.output_folder = output_folder\n",
    "    \n",
    "    def technical(self):\n",
    "\n",
    "        ## Run a Screaming Frog Crawl on the Command Line / Prompt to return technical issues \n",
    "        \n",
    "        sf_crawl = os.system('cd \"C:\\Program Files (x86)\\Screaming Frog SEO Spider\" && ScreamingFrogSEOSpiderCli.exe \\\n",
    "        --crawl {} --headless --output-folder {} --export-format \"csv\"  \\\n",
    "        --bulk-export \"Response Codes:Redirection (JavaScript) Inlinks,Links:All Outlinks,\\\n",
    "        Directives:Nofollow Inlinks,Response Codes:Redirection (Meta Refresh) Inlinks\" --export-tabs \\\n",
    "        \"Internal:All,Structured Data:All,Canonicals:All\" \\\n",
    "        --save-report \"Redirects:Redirect Chains\"\\\n",
    "        && cd {} && rename internal_all.csv internal_all_.csv'.format(website, output_folder))\n",
    "        \n",
    "        df = pd.read_csv(output_folder + '\\internal_all_.csv')\n",
    "\n",
    "        ### Find all the Canonical Link Elements that are non-200 status codes\n",
    "        canonicals = df[(df['Canonical Link Element 1'].notna()) & (df['Status Code'] == 200) ]\n",
    "        \n",
    "        for y in list(canonicals['Canonical Link Element 1']):\n",
    "                with open(output_folder + '\\\\canonical_status_code_check.txt', \"a\") as output:\n",
    "                    output.write(y + '\\n') \n",
    "        \n",
    "        canonical_list = output_folder + '\\\\canonical_status_code_check.txt'\n",
    "        crawl_canonicals = os.system('cd \"C:\\Program Files (x86)\\Screaming Frog SEO Spider\" && ScreamingFrogSEOSpiderCli.exe \\\n",
    "        --crawl-list {} --headless --output-folder {} --export-format \"csv\"  --export-tabs \\\n",
    "        \"Internal:All\"  && cd {} && rename internal_all.csv canonical_status_code.csv'.format(canonical_list,output_folder,output_folder))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Save outlink Destination URLS to a text file\n",
    "        \n",
    "        outlinks = pd.read_csv(output_folder +\"\\/all_outlinks.csv\")\n",
    "        \n",
    "        # Create a list of all the 301 redirects and run a crawl on them (this is to find the final destination URL)\n",
    "        outlinks[outlinks['Status Code'] == 301]['Destination']\n",
    "        for y in list(outlinks[outlinks['Status Code'] == 301]['Destination'].unique().astype(str)):\n",
    "            with open(output_folder + \"\\outlinks_status_code.txt\", \"a\", encoding='utf-8') as output:\n",
    "                output.write(y + '\\n')   \n",
    "        \n",
    "        final_destination_urls = output_folder + '\\\\outlinks_status_code.txt'\n",
    "     \n",
    "        crawl_301s = os.system('cd \"C:\\Program Files (x86)\\Screaming Frog SEO Spider\" && ScreamingFrogSEOSpiderCli.exe \\\n",
    "        --crawl-list {} --headless --output-folder {} --export-format \"csv\"  --export-tabs \\\n",
    "        \"Internal:All\"  && cd {} && rename internal_all.csv final_destination_urls.csv'.format(final_destination_urls,output_folder,output_folder))\n",
    "         \n",
    "\n",
    "        ### Create DataFrame listing out Non-Indexable URLs \n",
    "        non_indexable_urls = df[(df['Canonical Link Element 1'].isna() | df['Canonical Link Element 1'] == df['Address']) & (df['Indexability'] == 'Non-Indexable') & (df['Status Code'] == 200)]\n",
    "\n",
    "\n",
    "        #### Create DataFrame for all 200 status code pages with non-self referencing Canonicals\n",
    "        non_self_referencing_canonicals = df[(df['Canonical Link Element 1'] != df['Address']) & (df['Status Code'] == 200)]\n",
    "\n",
    "\n",
    "        ### Create DataFrame listing out Missing Headings \n",
    "        multiple_h1s = df[df['H1-2'].notna()]\n",
    "\n",
    "        ### Create DataFrame listing out Missing Headings \n",
    "        missing_headings = df[(df['H1-1'].isna())  | (df['H2-1'].isna())]\n",
    "\n",
    "        ### Create DataFrame listing out  Structured Data for Errors\n",
    "        structured_data = pd.read_csv(output_folder + '\\\\structured_data_all.csv')\n",
    "        structured_data_errors = structured_data[structured_data['Errors'] > 0]\n",
    "\n",
    "\n",
    "        ### Redirect Chains \n",
    "        redirect_chains  = pd.read_csv(output_folder + '\\\\redirect_chains.csv')\n",
    "\n",
    "\n",
    "        ### JavaScript Redirects \n",
    "        js_redirects  = pd.read_csv(output_folder + '\\\\redirection_(javascript)_inlinks.csv')\n",
    "\n",
    "\n",
    "        ### Meta Refreshes\n",
    "        meta_refreshes  = pd.read_csv(output_folder + '\\\\redirection_(meta_refresh)_inlinks.csv')\n",
    "\n",
    "\n",
    "        ### Canonical Status Codes\n",
    "\n",
    "        canonical_status_codes = pd.read_csv(output_folder + '\\\\canonical_status_code.csv')\n",
    "\n",
    "\n",
    "        ### the following code is fidning all non-indexable canonical links then joining it with the pages that are canonicalizing\n",
    "        ### to them\n",
    "\n",
    "        non_indexable_canonicals = canonical_status_codes[(canonical_status_codes['Status Code'] != 200) | (canonical_status_codes['Indexability'] != 'Indexable') ]\n",
    "        if len(non_indexable_canonicals) > 0:\n",
    "            non_indexable_canonicals = non_indexable_canonicals[['Address','Status Code','Indexability']] \n",
    "            non_indexable_canonicals.rename(columns = \n",
    "                                    {'Address':'Canonical Link Element 1','Status Code':'Canonical_Link_Status_Code',\n",
    "                                    'Indexability':'Canonical_Indexability'})\n",
    "\n",
    "            non_indexable_canonicals = non_indexable_canonicals.merge(df[['Address','Canonical Link Element 1']], how = 'left', on = 'Canonical Link Element 1')\n",
    "\n",
    "            ## create action column to callout non-indexable canonical link \n",
    "            non_indexable_canonicals['Action'] = 'Canonical Link is Non-Indexable'\n",
    "\n",
    "        outlinks = pd.read_csv(output_folder +\"\\/all_outlinks.csv\")\n",
    "\n",
    "        ### Merge Final Destination URLs\n",
    "        \n",
    "        final_destination_urls = pd.read_csv(output_folder + '\\\\final_destination_urls.csv')\n",
    "\n",
    "        final_destination_urls = final_destination_urls.rename(columns = {'Address':'Destination','Redirect URL':'Final Destination URL'})\n",
    "\n",
    "        outlinks.merge(final_destination_urls, how = 'left', on = 'Destination', inplace = True)\n",
    "        \n",
    "        outlinks = outlinks[outlinks['Destination'] != 200]\n",
    "\n",
    "        ### Save all Technical Issues to an Excel Sheet \n",
    "        with pd.ExcelWriter(output_folder  +  'Technical SEO Audit'  + '.xlsx') as writer:\n",
    "                df.to_excel(writer, sheet_name ='All URLs', index = False)\n",
    "                if len(non_indexable_urls) > 0:\n",
    "                    non_indexable_urls.to_excel(writer, sheet_name = 'Non_Indexable', index = False)\n",
    "                elif len(non_self_referencing_canonicals) > 0:\n",
    "                    non_self_referencing_canonicals.to_excel(writer, sheet_name = 'Non_Self_Ref', index = False) \n",
    "                elif len(multiple_h1s) > 0:\n",
    "                    multiple_h1s.to_excel(writer, sheet_name = 'Multiple_H1s', index = False) \n",
    "                elif len(missing_headings) > 0:\n",
    "                    missing_headings.to_excel(writer, sheet_name = 'Missing_Hs', index = False)         \n",
    "                elif len(structured_data_errors) > 0:\n",
    "                    structured_data_errors.to_excel(writer, sheet_name = 'Schema_Errors', index = False)   \n",
    "                elif len(redirect_chains) > 0:\n",
    "                    redirect_chains.to_excel(writer, sheet_name = 'Redirect_chains', index = False)   \n",
    "                elif len(js_redirects) > 0:\n",
    "                    js_redirects.to_excel(writer, sheet_name = 'JS_Redirects', index = False)                     \n",
    "                elif len(meta_refreshes) > 0:\n",
    "                    meta_refreshes.to_excel(writer, sheet_name = 'Meta_Refresh', index = False)                     \n",
    "                elif len(non_indexable_canonicals) > 0:\n",
    "                    non_indexable_canonicals.to_excel(writer, sheet_name = 'Non_Index_Canonicals', index = False) \n",
    "                elif len(outlinks) > 0:\n",
    "                    outlinks.to_excel(writer, sheet_name = 'Non_200_Pages', index = False) \n",
    "\n",
    "                    \n",
    "        print('Technical Crawl Complete')\n",
    "        \n",
    "    \n",
    "    def page_speed(self):\n",
    "        ps_command = os.system('cd \"C:\\Program Files (x86)\\Screaming Frog SEO Spider\" && ScreamingFrogSEOSpiderCli.exe --crawl {} --headless --output-folder {} --export-format \"csv\" --use-pagespeed --export-tabs \"Internal:HTML\" --save-report \\\n",
    "        \"PageSpeed:Minify JavaScript,\\\n",
    "        PageSpeed:JavaScript Coverage Summary,PageSpeed:Eliminate Render-Blocking Resources,\\\n",
    "        PageSpeed:PageSpeed Opportunities Summary,PageSpeed:CSS Coverage Summary,\\\n",
    "        PageSpeed:Serve Static Assets with an Efficient Cache Policy,\\\n",
    "        PageSpeed:Properly Size Images,PageSpeed:Defer Offscreen Images,\\\n",
    "        PageSpeed:Reduce Unused JavaScript,PageSpeed:Efficiently Encode Images,\\\n",
    "        PageSpeed:Serve Images in Next-Gen Formats,PageSpeed:Enable Text Compression,\\\n",
    "        PageSpeed:Minify CSS,PageSpeed:Minify JavaScript,PageSpeed:Reduce Unused CSS,\\\n",
    "        PageSpeed:Avoid Excessive DOM Size,PageSpeed:Reduce JavaScript Execution Time,\\\n",
    "        PageSpeed:Preload Key Requests,PageSpeed:Use Video Formats for Animated Content,\\\n",
    "        PageSpeed:Minimize Main-Thread Work,PageSpeed:Ensure Text Remains Visible During Webfont Load,\\\n",
    "        PageSpeed:Avoid Large Layout Shifts,\\\n",
    "        PageSpeed:Image Elements Do Not Have Explicit Width & Height\"'.format(website,output_folder))\n",
    "        \n",
    "        df = pd.read_csv(output_folder + '\\\\internal_html.csv')\n",
    "\n",
    "        \n",
    "        df = df[['Address','Title 1', 'Indexability', 'Performance Score', 'First Contentful Paint Time (ms)',\n",
    "                    'Speed Index Time (ms)', 'Largest Contentful Paint Time (ms)',\n",
    "                    'Time to Interactive (ms)', 'Total Blocking Time (ms)',\n",
    "                    'Cumulative Layout Shift', 'Total Size Savings (Bytes)',\n",
    "                    'Total Time Savings (ms)', 'Total Requests', 'Total Page Size (Bytes)',\n",
    "                    'HTML Size (Bytes)', 'HTML Count', 'Image Size (Bytes)', 'Image Count',\n",
    "                    'CSS Size (Bytes)', 'CSS Count', 'JavaScript Size (Bytes)',\n",
    "                    'JavaScript Count', 'Font Count','Font Size (Bytes)', \n",
    "                    'Media Size (Bytes)', 'Media Count', 'Other Size (Bytes)',\n",
    "                    'Other Count',  'Third Party Count','Third Party Size (Bytes)',\n",
    "                    'Core Web Vitals Assessment','Eliminate Render-Blocking Resources Savings (ms)',\n",
    "                    'Efficiently Encode Images Savings (ms)',\n",
    "                    'Defer Offscreen Images Savings (ms)',\n",
    "                    'Properly Size Images Savings (ms)',\n",
    "                    'Minify JavaScript Savings (ms)',\n",
    "                    'Minify CSS Savings (ms)',\n",
    "                    'Reduce Unused CSS Savings (ms)',\n",
    "                    'Reduce Unused JavaScript Savings (ms)',\n",
    "                    'Serve Images in Next-Gen Formats Savings (ms)',\n",
    "                    'Preconnect to Required Origins Savings (ms)',\n",
    "                    'Enable Text Compression Savings (ms)',\n",
    "                    'Server Response Times (TTFB) (ms)', 'Multiple Redirects Savings (ms)',\n",
    "                    'Preload Key Requests Savings (ms)',\n",
    "                    'Use Video Format for Animated Images Savings (ms)',\n",
    "                    'Total Image Optimization Savings (ms)',\n",
    "                    'Crawl Timestamp']]\n",
    "\n",
    "            # Combine CSVs into 1 single excel File\n",
    "            \n",
    "            dom_size = pd.read_csv(output_folder + '\\\\avoid_excessive_dom_size_report.csv')\n",
    "            css_summary = pd.read_csv(output_folder + '\\\\css_coverage_summary.csv')\n",
    "            defer_offscreen = pd.read_csv(output_folder + '\\\\defer_offscreen_images_report.csv')\n",
    "            encode_images = pd.read_csv(output_folder + '\\\\efficiently_encode_images_report.csv')\n",
    "            render_blocking = pd.read_csv(output_folder + '\\\\eliminate_render_blocking_resources_report.csv')\n",
    "            text_compression = pd.read_csv(output_folder + '\\\\enable_text_compression_report.csv')\n",
    "            text_remains_visible = pd.read_csv(output_folder + '\\\\ensure_text_remains_visible_during_webfont_load_report.csv')\n",
    "            minify_css_report = pd.read_csv(output_folder + '\\\\minify_css_report.csv')\n",
    "            minify_javascript_report = pd.read_csv(output_folder + '\\\\minify_javascript_report.csv')\n",
    "            js_coverage_summary = pd.read_csv(output_folder + '\\\\js_coverage_summary.csv')\n",
    "            minimize_main_thread_work_report = pd.read_csv(output_folder + '\\\\minimize_main_thread_work_report.csv')\n",
    "            pagespeed_opportunities_summary = pd.read_csv(output_folder + '\\\\pagespeed_opportunities_summary.csv')\n",
    "            preload_key_requests_report = pd.read_csv(output_folder + '\\\\preload_key_requests_report.csv')\n",
    "            properly_size_images_report = pd.read_csv(output_folder + '\\\\properly_size_images_report.csv')\n",
    "            reduce_javascript_execution_time_report = pd.read_csv(output_folder + '\\\\reduce_javascript_execution_time_report.csv')\n",
    "            reduce_unused_css_report = pd.read_csv(output_folder + '\\\\reduce_unused_css_report.csv')\n",
    "            reduce_unused_javascript_report = pd.read_csv(output_folder + '\\\\reduce_unused_javascript_report.csv')\n",
    "            serve_images_in_next_gen_formats_report = pd.read_csv(output_folder + '\\\\serve_images_in_next_gen_formats_report.csv')\n",
    "            serve_static_assets_with_an_efficient_cache_policy_report = pd.read_csv(output_folder + '\\\\serve_static_assets_with_an_efficient_cache_policy_report.csv')\n",
    "            use_video_formats_for_animated_content_report = pd.read_csv(output_folder + '\\\\use_video_formats_for_animated_content_report.csv')\n",
    "            avoid_large_layout_shifts = pd.read_csv(output_folder + '\\\\avoid_large_layout_shifts_report.csv')\n",
    "            image_elements_do_not_have_explicit_width_height_report = pd.read_csv(output_folder + '\\\\image_elements_do_not_have_explicit_width_&_height_report.csv')\n",
    "\n",
    "            ## save Page Speed Issues to an Excel Sheet\n",
    "            with pd.ExcelWriter(output_folder  +  ' Page Speed Audit'  + '.xlsx') as writer:\n",
    "                html.to_excel(writer, sheet_name ='All URLs', index = False)\n",
    "                if len(page_speed_aggregation) > 0:\n",
    "                    page_speed_aggregation.to_excel(writer, sheet_name = 'Page_Speed Opportunities', index = False)\n",
    "                if len(text_remains_visible) > 0:\n",
    "                    text_remains_visible.to_excel(writer, sheet_name = 'Text remains Visible ', index = False)\n",
    "                if len(text_compression) > 0:\n",
    "                    text_compression.to_excel(writer, sheet_name = 'Text Compression', index = False)\n",
    "                if len(serve_static_assets_with_an_efficient_cache_policy_report) > 0:\n",
    "                    serve_static_assets_with_an_efficient_cache_policy_report.to_excel(writer, sheet_name = 'Efficient Cache Policy', index = False)\n",
    "                if len(render_blocking) > 0:\n",
    "                    render_blocking.to_excel(writer, sheet_name = 'Render Blocking Res.', index = False)\n",
    "                if len(minify_css_report) > 0:\n",
    "                    minify_css_report.to_excel(writer, sheet_name = 'Minify CSS', index = False)\n",
    "                if len(minify_javascript_report) > 0:\n",
    "                    minify_javascript_report.to_excel(writer, sheet_name = 'Minify JS', index = False)\n",
    "                if len(reduce_javascript_execution_time_report) > 0:\n",
    "                    reduce_javascript_execution_time_report.to_excel(writer, sheet_name = 'Reduce JS Execution', index = False)\n",
    "                if len(reduce_unused_css_report) > 0:\n",
    "                    reduce_unused_css_report.to_excel(writer, sheet_name = 'Reduce Unused CSS', index = False)\n",
    "                if len(reduce_unused_javascript_report) > 0:\n",
    "                    reduce_unused_javascript_report.to_excel(writer, sheet_name = 'Reduce Unused JS', index = False)\n",
    "                if len(minimize_main_thread_work_report) > 0:\n",
    "                    minimize_main_thread_work_report.to_excel(writer, sheet_name = 'Minimize_Main_Thread', index = False)\n",
    "                if len(use_video_formats_for_animated_content_report) > 0:\n",
    "                    use_video_formats_for_animated_content_report.to_excel(writer, sheet_name = 'Use Video Format for Animation', index = False)\n",
    "                if len(preload_key_requests_report) > 0:\n",
    "                    preload_key_requests_report.to_excel(writer, sheet_name = 'Preload_Key_Reqs', index = False)               \n",
    "                if len(properly_size_images_report) > 0:\n",
    "                    properly_size_images_report.to_excel(writer, sheet_name = 'Properly_Size_Images', index = False)\n",
    "                if len(encode_images) > 0:\n",
    "                    encode_images.to_excel(writer, sheet_name = 'Encode_Images', index = False)\n",
    "                if len(defer_offscreen) > 0:\n",
    "                    defer_offscreen.to_excel(writer, sheet_name = 'Defer Offscreen Images', index = False)\n",
    "                if len(image_elements_do_not_have_explicit_width_height_report) > 0 :\n",
    "                    image_elements_do_not_have_explicit_width_height_report.to_excel(writer, sheet_name = 'Images Missing Width & Height', index = False)\n",
    "                if len(avoid_large_layout_shifts) > 0 :\n",
    "                    avoid_large_layout_shifts.to_excel(writer, sheet_name = 'Avoid Large Layout Shifts', index = False)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical SEO Audit\n",
    "- the code below will run Screaming Frog Crawls via the command line / prompt, then save all the technical issues in an excel file\n",
    "- Here are the issues we check for:\n",
    "    - Non-Indexable URLs (200s)\n",
    "    - Non-Self Referencing Canonicals \n",
    "    - Pages with Multiple H-1s\n",
    "    - Pages with Missing Headings (H-1s and H-2s)\n",
    "    - Structured Data Errors  \n",
    "    - Redirect Chains\n",
    "    - JavaScript Redirects\n",
    "    - URLs that Canonicalize to Non-Indexable Pages\n",
    "    - Meta Refreshes\n",
    "    - All Non-200 Status Code Outlinks (For any 301 redirects, we ran an additional crawl to get the final destination URLs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r'c:\\users\\your_output_folder'\n",
    "website = 'https://www.yourwebsite.com/'\n",
    "sf_crawl = sf_crawl_audit(website, output_folder)\n",
    "\n",
    "\n",
    "sf_crawl.technical()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Data\n",
    "- In addition to saving all the data in an excel file (Which you can open up afterwards), we can also visualize the data in this jupyter notebook by running the rest of the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_folder + '\\\\internal_all.csv')\n",
    "\n",
    "### Create DataFrame listing out Non-Indexable URLs \n",
    "non_indexable_urls = df[(df['Canonical Link Element 1'].isna() | df['Canonical Link Element 1'] == df['Address']) & (df['Indexability'] == 'Non-Indexable') & (df['Status Code'] == 200)]\n",
    "\n",
    "\n",
    "#### Create DataFrame for all 200 status code pages with non-self referencing Canonicals\n",
    "non_self_referencing_canonicals = df[(df['Canonical Link Element 1'] != df['Address']) & (df['Status Code'] == 200)]\n",
    "\n",
    "\n",
    "### Create DataFrame listing out Missing Headings \n",
    "multiple_h1s = df[df['H1-2'].notna()]\n",
    "\n",
    "### Create DataFrame listing out Missing Headings \n",
    "missing_headings = df[(df['H1-1'].isna())  | (df['H2-1'].isna())]\n",
    "\n",
    "### Create DataFrame listing out  Structured Data for Errors\n",
    "structured_data = pd.read_csv(output_folder + '\\\\structured_data_all.csv')\n",
    "structured_data_errors = structured_data[structured_data['Errors'] > 0]\n",
    "\n",
    "\n",
    "### Redirect Chains \n",
    "redirect_chains  = pd.read_csv(output_folder + '\\\\redirect_chains.csv')\n",
    "\n",
    "\n",
    "### JavaScript Redirects \n",
    "js_redirects  = pd.read_csv(output_folder + '\\\\redirection_(javascript)_inlinks.csv')\n",
    "\n",
    "\n",
    "### Meta Refreshes\n",
    "meta_refreshes  = pd.read_csv(output_folder + '\\\\redirection_(meta_refresh)_inlinks.csv')\n",
    "\n",
    "\n",
    "### Canonical Status Codes\n",
    "\n",
    "canonical_status_codes = pd.read_csv(output_folder + '\\\\canonical_status_code.csv')\n",
    "\n",
    "\n",
    "### the following code is fidning all non-indexable canonical links then joining it with the pages that are canonicalizing\n",
    "### to them\n",
    "\n",
    "non_indexable_canonicals = canonical_status_codes[(canonical_status_codes['Status Code'] != 200) | (canonical_status_codes['Indexability'] != 'Indexable') ]\n",
    "if len(non_indexable_canonicals) > 0:\n",
    "    non_indexable_canonicals = non_indexable_canonicals[['Address','Status Code','Indexability']] \n",
    "    non_indexable_canonicals.rename(columns = \n",
    "                            {'Address':'Canonical Link Element 1','Status Code':'Canonical_Link_Status_Code',\n",
    "                            'Indexability':'Canonical_Indexability'})\n",
    "\n",
    "    non_indexable_canonicals = non_indexable_canonicals.merge(df[['Address','Canonical Link Element 1']], how = 'left', on = 'Canonical Link Element 1')\n",
    "    \n",
    "    ## create action column to callout non-indexable canonical link \n",
    "    non_indexable_canonicals['Action'] = 'Canonical Link is Non-Indexable'\n",
    "\n",
    "### 301 Redirects   \n",
    "outlinks = pd.read_csv(output_folder +\"\\/all_outlinks.csv\")\n",
    "\n",
    "final_destination_urls = pd.read_csv(output_folder + '\\\\final_destination_urls.csv')\n",
    "\n",
    "final_destination_urls = final_destination_urls.rename(columns = {'Address':'Destination','Redirect URL':'Final Destination URL'})\n",
    "\n",
    "outlinks.merge(final_destination_urls, how = 'left', on = 'Destination', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Indexable URLs\n",
    "- Here are all your non-indexable 200 status Code URLs (Not Canonicalizing to another URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_indexable_urls.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Heading Tags\n",
    "- Here are pages with missing Headings Tags (H-1s and H-2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_headings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL has a non-self referencing Canonical Tag\n",
    "- URL's status code is 200, but it contains a non-self referencing canonical tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_self_referencing_canonicals.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple H-1 tags\n",
    "- URLs have more than 1 H-1 tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_h1s.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Data Issues \n",
    "- These URLs have Structured Data Errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data_errors.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redirect Chains \n",
    "- Here are all the redirect chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_chains.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JavaScript Redirects \n",
    "- Here are all the JS Redirects that should be cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_redirects.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta Refreshes\n",
    "- Here are all the Meta Refreshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_refreshes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canonical Status Codes\n",
    "- Here are all the canonicals that are not indexable\n",
    "- we joined the non-indexable canonicals with the pages canonicalizing to them so you can quickly fix which pages need their canonicals updated! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_indexable_canonicals.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non 200 Status Code Outlinks\n",
    "- Here are all the pages that we link to returning a non 200 status code \n",
    "- For any 301s, we also ran a crawl to return the \"Final Destination URL\", so you can quickly swap out the redirect link for the final destination URL. \n",
    "- Screaming Frog's Outlinks Export is great because it tells you where the link can be found (X-Path), the anchor used and the type of link (CSS, JS, Image, HTML, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlinks[outlinks['Status Code'] != 200].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Speed Audit\n",
    "- the code below will run a page speed audit via API Key (using Screaming Frog) via the command line / prompt, then save all the technical issues in an excel file\n",
    "- Here are the issues we check for:\n",
    "    - Page Speed Metrics at the URL Level (Including CWV)\n",
    "    - Render-blocking resources\n",
    "    - Assets that lengthen their cache policy \n",
    "    - Images that aren't properly sized\n",
    "    - Offscreen Images that need to be deferred  \n",
    "    - Images not in Next-Gen Format\n",
    "    - Minified JS, CSS\n",
    "    - Unused JS and CSS \n",
    "    - JS with Long Execution Time \n",
    "    - Elements avoiding layout shifts\n",
    "    - Image elemnts that don't have explicit height and Width \n",
    "    - Excessive Dom Size\n",
    "- Sf also has a summary tab that shows where we can improve overall \n",
    "\n",
    "#### you can prioritize your efforts based on the URLs likely to drive the most Revenue (Unfortunately, I do not have that data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_crawl.page_speed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Data\n",
    "- In addition to saving all the data in an excel file (Which you can open up afterwards), we can also visualize the summary data below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS Summary \n",
    "- Here is the CSS Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_summary = pd.read_csv(output_folder + '\\\\css_coverage_summary.csv')\n",
    "css_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JavaScript Summary\n",
    "- Here is the JS Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_coverage_summary = pd.read_csv(output_folder + '\\\\js_coverage_summary.csv')\n",
    "js_coverage_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
