{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram Analysis\n",
    "\n",
    "### Initial Setup \n",
    "- This script uses the GSC API \n",
    "- If you don't have your GSC API Key, watch this video from Jean Chouinard first: https://www.youtube.com/watch?v=-uy4L4P1Ujs&t=4s\n",
    "- This code was built off June Tao Ching's GSC code and was modified to audit my content accordingly\n",
    "    - Source: https://towardsdatascience.com/access-google-search-console-data-on-your-site-by-using-python-3c8c8079d6f8\n",
    "\n",
    "### What does the code do?\n",
    "\n",
    "- The following code pulls Keyword Data from GSC and then runs an ngram analysis\n",
    "\n",
    "### What is an Ngram Analysis?\n",
    "- \"An n-gram is a collection of n successive items in a text document that may include words, numbers, symbols, and punctuation.\" - https://www.mathworks.com/discovery/ngram.html\n",
    "\n",
    "### How can Ngrams be used for SEO?\n",
    "- You can use Ngrams to analyze keywords to find patterns patterns, topic cluster / internal linking ideas \n",
    "- Ngrams can used to analyze your page titles, urls, keywords, H-1s, anchor text (internal and external) to find specific patterns \n",
    "\n",
    "#### Examples:\n",
    "- Ngrams on GSC Keywords to find Topic Clusters / Internal Linking opportunities\n",
    "- Ngrams on competitor Keyword data / titles / H-1s to find other keywords / topics to build out\n",
    "- Ngrams on competitors titles (best by links (ahrefs report)) to determine which pages are generating the most backlinks\n",
    "- Ngrams on GSC Keywords / page titles to analyze the impact of an algorithm update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "import pandas as pd \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from apiclient.discovery import build\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# GSC Class\n",
    "\n",
    "class gsc_api:\n",
    "\n",
    "    def __init__(self,website,start_date,end_date):\n",
    "\n",
    "        self.website = website\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "\n",
    "\n",
    "    #How Script gets access to Reddit via developer API.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def gsc_kw(self):\n",
    "\n",
    "        SITE_URL = self.website\n",
    "\n",
    "        OAUTH_SCOPE = ('https://www.googleapis.com/auth/webmasters.readonly', 'https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "        # Redirect URI for installed apps\n",
    "        REDIRECT_URI = 'urn:ietf:wg:oauth:2.0:oob'\n",
    "        \n",
    "        \n",
    "        # You must edit gsc_credentials and pickled_credentials to include YOUR username\n",
    "        gsc_credentials = r'your credentials '\n",
    "        \n",
    "        # where your pickled credential will be stored\n",
    "\n",
    "        pickled_credentials = r'c:\\users\\your_username\\desktop\\pickled_credential'\n",
    "\n",
    "\n",
    "        try:\n",
    "            credentials = pickle.load(open(pickled_credentials  + \".pickle\", \"rb\"))\n",
    "        except (OSError, IOError) as e:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(gsc_credentials, scopes=OAUTH_SCOPE)\n",
    "            credentials = flow.run_console()\n",
    "            pickle.dump(credentials, open(pickled_credentials  + \".pickle\", \"wb\"))\n",
    "\n",
    "            # Connect to Search Console Service using the credentials \n",
    "        webmasters_service = build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "        maxRows = 25000\n",
    "        i = 0\n",
    "        output_rows = []\n",
    "        start_date = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        def date_range(start_date, end_date, delta=timedelta(days=1)):\n",
    "\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                yield current_date\n",
    "                current_date += delta\n",
    "        print('script start date:', start_date)\n",
    "\n",
    "        for date in date_range(start_date, end_date):\n",
    "            date = date.strftime(\"%Y-%m-%d\")\n",
    "            i = 0\n",
    "            while True:\n",
    "\n",
    "                request = {\n",
    "                    'startDate' : date,\n",
    "                    'endDate' : date,\n",
    "                    'dimensions' : [\"page\",'query'],\n",
    "                    \"searchType\": \"Web\",\n",
    "                    'rowLimit' : maxRows,\n",
    "                    'startRow' : i * maxRows\n",
    "                }\n",
    "\n",
    "                response = webmasters_service.searchanalytics().query(siteUrl = SITE_URL, body=request).execute()\n",
    "                if response is None:\n",
    "                    break\n",
    "                if 'rows' not in response:\n",
    "                    break\n",
    "                else:\n",
    "                    for row in response['rows']:\n",
    "                        page = row['keys'][0]\n",
    "                        keyword = row['keys'][1]\n",
    "                        output_row = [ page,keyword, row['clicks'], row['impressions'], row['ctr'], row['position']]\n",
    "                        output_rows.append(output_row)\n",
    "                    i = i + 1\n",
    "        print('script end date:', end_date)\n",
    "\n",
    "        df = pd.DataFrame(output_rows, columns=['Address','Keyword', 'Clicks', 'Impressions', 'CTR',  'Average Position'])\n",
    "        df = df.groupby(['Address','Keyword']).agg({'Clicks':'sum','Impressions':'sum','Average Position':'mean'}).reset_index()\n",
    "        df['CTR'] = df['Clicks'] / df['Impressions'] \n",
    "        return df\n",
    "\n",
    "    \n",
    "# Ngram Class\n",
    "class n_gram:\n",
    "\n",
    "    def __init__(self,data):\n",
    "\n",
    "        self.data = data\n",
    "        # self.column = column\n",
    "        \n",
    "    def generate_N_grams(self,text,ngram=1):\n",
    "      self.text = text\n",
    "      self.ngram = ngram\n",
    "      words=[word for word in text.split(\" \")]  \n",
    "      print(\"Sentence after removing stopwords:\",words)\n",
    "      temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "      ans=[' '.join(ngram) for ngram in temp]\n",
    "      return ans\n",
    "        \n",
    "    def n_gram_function(self , s):  \n",
    "        data = self.data\n",
    "        # column = self.column\n",
    "        gram = defaultdict(int)\n",
    "        for text in data['Keyword']:\n",
    "              for word in self.generate_N_grams(text,s):\n",
    "                gram[word]+=1\n",
    "        gram = pd.DataFrame(sorted(gram.items(),key=lambda x:x[1],reverse=True))\n",
    "        return gram\n",
    "    \n",
    "    \n",
    "    ## Returns Unigram \n",
    "    def unigram(self):\n",
    "        unigram = self.n_gram_function(1)\n",
    "        return unigram\n",
    "    \n",
    "    ## Returns Bigram \n",
    "    def bigram(self):\n",
    "        bigram = self.n_gram_function(2)\n",
    "        return bigram\n",
    "\n",
    "    ## Returns Trigram \n",
    "    def trigram(self):\n",
    "        trigram = self.n_gram_function(3)\n",
    "        return trigram\n",
    "\n",
    "    ## Returns Quadgram \n",
    "    def quadgram(self):\n",
    "        quadgram = self.n_gram_function(4)\n",
    "        return quadgram\n",
    "    \n",
    "    ## Returns fivegram \n",
    "    def quintgram(self):\n",
    "        quintgram = self.n_gram_function(5)\n",
    "        return quintgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following code pulls data from the GSC API and then runs an Ngram analysis on your keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc_data = gsc_api('https://yourwebsite.com/','2022-10-01','2022-10-31')\n",
    "df = gsc_data.gsc_kw()\n",
    "ngram = n_gram(df)\n",
    "\n",
    "#returns trigram analysis on keywords\n",
    "trigram = ngram.trigram()\n",
    "\n",
    "#returns bigram analysis on GSC Keywords\n",
    "bigram = ngram.trigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
