{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEO Content Audit \n",
    "\n",
    "### Initial Setup \n",
    "- This script uses the GSC API and runs a Screaming Frog crawl via the command line\n",
    "- If you don't have your GSC API Key, watch this video from Jean Chouinard first: \n",
    "    - https://www.youtube.com/watch?v=-uy4L4P1Ujs&t=4s\n",
    "- You will also need a paid subscription to Screaming Frog before proceeding  \n",
    "- This code was built off June Tao Ching's GSC code and was modified to audit my content accordingly\n",
    "    - Source: https://towardsdatascience.com/access-google-search-console-data-on-your-site-by-using-python-3c8c8079d6f8\n",
    "\n",
    "### What does the code do?\n",
    "\n",
    "- The following code runs a crawl on your website then pulls data from GSC's API and merges it together\n",
    "- It pulls in data for X time range and compares that data to a previous time period (specified), then checks for the following:\n",
    "    - Low Traffic Pages (< 50 Clicks)\n",
    "    - Pages that lost traffic and saw a drop in average position\n",
    "    - Low Hanging Fruit (Pages Ranking on Page 1 for their main keyword (by clicks), but not in the top 2 positions)\n",
    "    - Striking Distance(Pages Ranking on Page 2 for their main keyword (by clicks))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from apiclient.discovery import build\n",
    "\n",
    "class content_audit:\n",
    "\n",
    "    def __init__(self,website,output_folder,start_date,end_date, prev_start_date, prev_end_date):\n",
    "\n",
    "        self.website = website\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.prev_start_date = prev_start_date\n",
    "        self.prev_end_date = prev_end_date\n",
    "        self.output_folder = output_folder\n",
    "    \n",
    "\n",
    "    def url_level_data(self):\n",
    "\n",
    "        SITE_URL = self.website\n",
    "\n",
    "        OAUTH_SCOPE = ('https://www.googleapis.com/auth/webmasters.readonly', 'https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "        # Redirect URI for installed apps\n",
    "        REDIRECT_URI = 'urn:ietf:wg:oauth:2.0:oob'\n",
    "        \n",
    "        # You must edit gsc_credentials and pickled_credentials to include YOUR username\n",
    "        gsc_credentials = r'your credentials '\n",
    "        \n",
    "        # where your pickled credential will be stored\n",
    "\n",
    "        pickled_credentials = r'c:\\users\\your_username\\desktop\\pickled_credential'\n",
    "\n",
    "\n",
    "        try:\n",
    "            credentials = pickle.load(open(pickled_credentials  + \".pickle\", \"rb\"))\n",
    "        except (OSError, IOError) as e:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(gsc_credentials, scopes=OAUTH_SCOPE)\n",
    "            credentials = flow.run_console()\n",
    "            pickle.dump(credentials, open(pickled_credentials  + \".pickle\", \"wb\"))\n",
    "\n",
    "            # Connect to Search Console Service using the credentials \n",
    "        webmasters_service = build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "        maxRows = 25000\n",
    "        i = 0\n",
    "        output_rows = []\n",
    "        start_date = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        def date_range(start_date, end_date, delta=timedelta(days=1)):\n",
    "\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                yield current_date\n",
    "                current_date += delta\n",
    "        print('script start date:', start_date)\n",
    "\n",
    "        for date in date_range(start_date, end_date):\n",
    "            date = date.strftime(\"%Y-%m-%d\")\n",
    "            i = 0\n",
    "            while True:\n",
    "\n",
    "                request = {\n",
    "                    'startDate' : date,\n",
    "                    'endDate' : date,\n",
    "                    'dimensions' : [\"page\"],\n",
    "                    \"searchType\": \"Web\",\n",
    "                    'rowLimit' : maxRows,\n",
    "                    'startRow' : i * maxRows\n",
    "                }\n",
    "\n",
    "                response = webmasters_service.searchanalytics().query(siteUrl = SITE_URL, body=request).execute()\n",
    "                if response is None:\n",
    "                    break\n",
    "                if 'rows' not in response:\n",
    "                    break\n",
    "                else:\n",
    "                    for row in response['rows']:\n",
    "                        page = row['keys'][0]\n",
    "                        output_row = [page, row['clicks'], row['impressions'], row['position']]\n",
    "                        output_rows.append(output_row)\n",
    "                    i = i + 1\n",
    "        print('script end date:', end_date)\n",
    "\n",
    "        df = pd.DataFrame(output_rows, columns=['Address', 'URL Clicks', 'URL Impressions', 'URL Average Position'])\n",
    "        df = df.groupby(['Address']).agg({'URL Clicks':'sum','URL Impressions':'sum','URL Average Position':'mean'}).reset_index()\n",
    "        df['URL CTR'] = df['URL Clicks'] / df['URL Impressions'] \n",
    "        return df\n",
    "\n",
    "    def prev_url_level_data(self):\n",
    "\n",
    "        SITE_URL = self.website\n",
    "\n",
    "        OAUTH_SCOPE = ('https://www.googleapis.com/auth/webmasters.readonly', 'https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "        # Redirect URI for installed apps\n",
    "        REDIRECT_URI = 'urn:ietf:wg:oauth:2.0:oob'\n",
    "        \n",
    "        # You must edit gsc_credentials and pickled_credentials to include YOUR username\n",
    "        gsc_credentials = r'your credentials '\n",
    "        \n",
    "        # where your pickled credential will be stored\n",
    "\n",
    "        pickled_credentials = r'c:\\users\\your_username\\desktop\\pickled_credential'\n",
    "\n",
    "\n",
    "        try:\n",
    "            credentials = pickle.load(open(pickled_credentials  + \".pickle\", \"rb\"))\n",
    "        except (OSError, IOError) as e:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(gsc_credentials, scopes=OAUTH_SCOPE)\n",
    "            credentials = flow.run_console()\n",
    "            pickle.dump(credentials, open(pickled_credentials  + \".pickle\", \"wb\"))\n",
    "\n",
    "            # Connect to Search Console Service using the credentials \n",
    "        webmasters_service = build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "        maxRows = 25000\n",
    "        i = 0\n",
    "        output_rows = []\n",
    "        prev_start_date = datetime.strptime(self.prev_start_date, \"%Y-%m-%d\")\n",
    "        prev_end_date = datetime.strptime(self.prev_end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        def date_range(start_date, end_date, delta=timedelta(days=1)):\n",
    "\n",
    "            current_date = prev_start_date\n",
    "            while current_date <= prev_end_date:\n",
    "                yield current_date\n",
    "                current_date += delta\n",
    "        print('script start date:', prev_start_date)\n",
    "\n",
    "        for date in date_range(prev_start_date, prev_end_date):\n",
    "            date = date.strftime(\"%Y-%m-%d\")\n",
    "            i = 0\n",
    "            while True:\n",
    "\n",
    "                request = {\n",
    "                    'startDate' : date,\n",
    "                    'endDate' : date,\n",
    "                    'dimensions' : [\"page\"],\n",
    "                    \"searchType\": \"Web\",\n",
    "                    'rowLimit' : maxRows,\n",
    "                    'startRow' : i * maxRows\n",
    "                }\n",
    "\n",
    "                response = webmasters_service.searchanalytics().query(siteUrl = SITE_URL, body=request).execute()\n",
    "                if response is None:\n",
    "                    break\n",
    "                if 'rows' not in response:\n",
    "                    break\n",
    "                else:\n",
    "                    for row in response['rows']:\n",
    "                        page = row['keys'][0]\n",
    "                        output_row = [page, row['clicks'], row['impressions'], row['position']]\n",
    "                        output_rows.append(output_row)\n",
    "                    i = i + 1\n",
    "        print('script end date:', prev_end_date)\n",
    "\n",
    "        df = pd.DataFrame(output_rows, columns=['Address', 'URL Clicks Prev', 'URL Impressions Prev', 'URL Average Position Prev'])\n",
    "        df = df.groupby(['Address']).agg({'URL Clicks Prev':'sum','URL Impressions Prev':'sum','URL Average Position Prev':'mean'}).reset_index()\n",
    "        df['URL CTR Prev'] = df['URL Clicks Prev'] / df['URL Impressions Prev'] \n",
    "        return df    \n",
    "\n",
    "    def gsc_kw(self):\n",
    "\n",
    "        SITE_URL = self.website\n",
    "\n",
    "        OAUTH_SCOPE = ('https://www.googleapis.com/auth/webmasters.readonly', 'https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "        # Redirect URI for installed apps\n",
    "        REDIRECT_URI = 'urn:ietf:wg:oauth:2.0:oob'\n",
    "        \n",
    "        # You must edit gsc_credentials and pickled_credentials to include YOUR username\n",
    "        gsc_credentials = r'your credentials '\n",
    "        \n",
    "        # where your pickled credential will be stored\n",
    "\n",
    "        pickled_credentials = r'c:\\users\\your_username\\desktop\\pickled_credential'\n",
    "\n",
    "\n",
    "        try:\n",
    "            credentials = pickle.load(open(pickled_credentials  + \".pickle\", \"rb\"))\n",
    "        except (OSError, IOError) as e:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(gsc_credentials, scopes=OAUTH_SCOPE)\n",
    "            credentials = flow.run_console()\n",
    "            pickle.dump(credentials, open(pickled_credentials  + \".pickle\", \"wb\"))\n",
    "\n",
    "            # Connect to Search Console Service using the credentials \n",
    "        webmasters_service = build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "        maxRows = 25000\n",
    "        i = 0\n",
    "        output_rows = []\n",
    "        start_date = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        def date_range(start_date, end_date, delta=timedelta(days=1)):\n",
    "\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                yield current_date\n",
    "                current_date += delta\n",
    "        print('script start date:', start_date)\n",
    "\n",
    "        for date in date_range(start_date, end_date):\n",
    "            date = date.strftime(\"%Y-%m-%d\")\n",
    "            i = 0\n",
    "            while True:\n",
    "\n",
    "                request = {\n",
    "                    'startDate' : date,\n",
    "                    'endDate' : date,\n",
    "                    'dimensions' : [\"page\",'query'],\n",
    "                    \"searchType\": \"Web\",\n",
    "                    'rowLimit' : maxRows,\n",
    "                    'startRow' : i * maxRows\n",
    "                }\n",
    "\n",
    "                response = webmasters_service.searchanalytics().query(siteUrl = SITE_URL, body=request).execute()\n",
    "                if response is None:\n",
    "                    break\n",
    "                if 'rows' not in response:\n",
    "                    break\n",
    "                else:\n",
    "                    for row in response['rows']:\n",
    "                        page = row['keys'][0]\n",
    "                        keyword = row['keys'][1]\n",
    "                        output_row = [ page,keyword, row['clicks'], row['impressions'], row['ctr'], row['position']]\n",
    "                        output_rows.append(output_row)\n",
    "                    i = i + 1\n",
    "        print('script end date:', end_date)\n",
    "\n",
    "        df = pd.DataFrame(output_rows, columns=['Address','Main Keyword', 'KW Clicks', 'KW Impressions', 'KW CTR',  'KW Average Position'])\n",
    "        df = df.groupby(['Address','Main Keyword']).agg({'KW Clicks':'sum','KW Impressions':'sum','KW Average Position':'mean'}).reset_index()\n",
    "        df['KW CTR'] = df['KW Clicks'] / df['KW Impressions'] \n",
    "        return df\n",
    "   \n",
    "    def screaming_frog_crawl(self):\n",
    "        website = self.website\n",
    "        output_folder = self.output_folder\n",
    "        sf_command = os.system('cd \"C:\\Program Files (x86)\\Screaming Frog SEO Spider\" && ScreamingFrogSEOSpiderCli.exe --crawl {} --headless --output-folder {} --export-tabs \"Internal:All\"'\\\n",
    "            .format(website,output_folder))\n",
    "    \n",
    "    ### Method calls GSC API and SF Method then joins the data \n",
    "    def url_clean_up(self):\n",
    "        url_data_now = self.url_level_data()\n",
    "        url_prev_data = self.prev_url_level_data()\n",
    "        sf_crawl = self.screaming_frog_crawl()\n",
    "        keyword_data = self.gsc_kw()\n",
    "        output_folder = self.output_folder\n",
    "        \n",
    "        url_data_gsc = url_data_now.merge(url_prev_data, how = 'left', on = 'Address')\n",
    "        url_data_gsc[['URL Clicks','URL Impressions', 'URL Clicks Prev','URL Impressions Prev','URL Average Position','URL Average Position Prev']] = url_data_gsc[['URL Clicks','URL Impressions', 'URL Clicks Prev','URL Impressions Prev','URL Average Position','URL Average Position Prev']].fillna(0)\n",
    "        url_data_gsc['URL Clicks Diff'] = url_data_gsc['URL Clicks'] -  url_data_gsc['URL Clicks Prev'] \n",
    "        url_data_gsc['URL Impressions Diff'] = url_data_gsc['URL Clicks'] -  url_data_gsc['URL Impressions Prev'] \n",
    "        url_data_gsc['URL Average Position Diff'] = url_data_gsc['URL Average Position Prev'] - url_data_gsc['URL Average Position']\n",
    "        \n",
    "        \n",
    "        keyword_data = keyword_data.sort_values(by = 'KW Clicks', ascending = False).groupby(['Address']).head(1)\n",
    "        \n",
    "        df = pd.read_csv(output_folder + '\\internal_all.csv') \n",
    "        df = df[df['Indexability'] == 'Indexable'][['Address','Title 1','H1-1','Status Code','Word Count']]\n",
    "        df = df.merge(url_data_gsc, how = 'left', on = 'Address')\n",
    "        df = df.merge(keyword_data, how = 'left', on = 'Address')\n",
    "        df[['URL Clicks','URL Impressions', 'URL Clicks Prev','URL Impressions Prev','URL Average Position','URL Average Position Prev']] = df[['URL Clicks','URL Impressions', 'URL Clicks Prev','URL Impressions Prev','URL Average Position','URL Average Position Prev']].fillna(0)\n",
    "        \n",
    "        \n",
    "        # Function checks for thin/low quality content, keywords that lost traffic, and keywords in striking distance\n",
    "        \n",
    "        def content_cleanup(df):\n",
    "            clicks_url =  df['URL Clicks']\n",
    "            impressions_url = df['URL Impressions']\n",
    "            clicks_diff_url = df['URL Clicks Diff']\n",
    "            avg_pos_diff_url = df['URL Average Position Diff']\n",
    "            main_keyword = df['Main Keyword'] \n",
    "            main_keyword_rank = df['KW Average Position'] \n",
    "            \n",
    "            word_count = df['Word Count']\n",
    "            \n",
    "            if clicks_url < 50 and impressions_url < 100 and word_count < 400:\n",
    "                return 'Delete or No Index'\n",
    "            elif clicks_url < 50 and impressions_url < 100 and word_count >= 400:\n",
    "                return 'Re-evaluate Content (Low Traffic)'\n",
    "            elif clicks_diff_url >= 100 and avg_pos_diff_url < 0:\n",
    "                return 'URL Lost Traffic over 3 months - Re-Evaluate Content'        \n",
    "            elif main_keyword_rank < 2 and main_keyword_rank > 0:\n",
    "                return 'Main Keyword is Ranking in Position 1 (Leave as is)'        \n",
    "            elif main_keyword_rank >= 2 and main_keyword_rank <= 10:\n",
    "                return 'Low Hanging Fruit - Main Keyword is Ranking on Page 1'    \n",
    "            elif main_keyword_rank >= 10 and main_keyword_rank <= 20:\n",
    "                return 'Striking Distance Content - Main Keyword is Ranking on Page 2'            \n",
    "        df['URL Action'] = df.apply(content_audit, axis = 1) \n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where you want to store your sf crawl\n",
    "output_folder = r'C:\\Users\\your_username\\Desktop\\Competitors'\n",
    "\n",
    "\n",
    "# Instantiate your class / Create Object \n",
    "# add in your website and date ranges\n",
    "content_audit_seo = content_audit('yourwebsite', output_folder, start_date = '2022-08-01', end_date = '2022-10-31', prev_start_date = '2022-05-01', prev_end_date = '2022-07-31')\n",
    "\n",
    "df = content_audit_seo.url_clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
