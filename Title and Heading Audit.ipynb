{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if your Main / Primary Keyword is not in your Title or H-1\n",
    "\n",
    "### Initial Setup \n",
    "- This script uses the GSC API and runs a Screaming Frog Crawl via the command line\n",
    "- If you don't have your GSC API Key, watch this video from Jean Chouinard first: https://www.youtube.com/watch?v=-uy4L4P1Ujs&t=4s\n",
    "- You will also need a paid subscription to Screaming Frog before proceeding \n",
    "- This code was built off June Tao Ching's GSC code and was modified to audit my content accordingly\n",
    "    - Source: https://towardsdatascience.com/access-google-search-console-data-on-your-site-by-using-python-3c8c8079d6f8\n",
    "\n",
    "### What does the code do?\n",
    "\n",
    "- The following code pulls keyword data from GSC and crawl data from screaming Frog \n",
    "- the purpose of running a crawl is to get H-1s, H-2s and Title tags for each URL\n",
    "- then it filters for only the top 5 keywords by clicks per URL \n",
    "- Lastly, it checks to see the percentage of each of the top 5 keywords in the title, H-1 and H-2\n",
    "- if you have a large site, you should prioritize according to traffic or revenue potential\n",
    "\n",
    "#### How to Analyze the Data?\n",
    "\n",
    "- I've found that updating title tags can turn around traffic pretty quickly especially if my URL is already ranking on page 1 or 2. \n",
    "- If the main keyword is not found in my title, I can quickly make updates \n",
    "- This code will catch keyword percentage in titles or H-1s regardless of the ordering (i.e. cancer symptoms or symptoms cancer)\n",
    "- you can then check titles that have less than a 75% keyword match (or whatever you see fit) and update your titles accordingly\n",
    "- We pull the top 5 keywords by clicks instead of just the top 1-2, so we're covering all our bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from apiclient.discovery import build\n",
    "\n",
    "class keyword_check:\n",
    "\n",
    "    def __init__(self,website,output_folder,start_date,end_date):\n",
    "\n",
    "        self.website = website\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.output_folder = output_folder\n",
    "    \n",
    "\n",
    "    def gsc_kw(self):\n",
    "\n",
    "        SITE_URL = self.website\n",
    "\n",
    "        OAUTH_SCOPE = ('https://www.googleapis.com/auth/webmasters.readonly', 'https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "        # Redirect URI for installed apps\n",
    "        REDIRECT_URI = 'urn:ietf:wg:oauth:2.0:oob'\n",
    "        \n",
    "        # You must edit gsc_credentials and pickled_credentials to include YOUR username\n",
    "        gsc_credentials = r'your credentials '\n",
    "        \n",
    "        # where your pickled credential will be stored\n",
    "\n",
    "        pickled_credentials = r'c:\\users\\your_username\\desktop\\pickled_credential'\n",
    "\n",
    "\n",
    "        try:\n",
    "            credentials = pickle.load(open(pickled_credentials  + \".pickle\", \"rb\"))\n",
    "        except (OSError, IOError) as e:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(gsc_credentials, scopes=OAUTH_SCOPE)\n",
    "            credentials = flow.run_console()\n",
    "            pickle.dump(credentials, open(pickled_credentials  + \".pickle\", \"wb\"))\n",
    "\n",
    "            # Connect to Search Console Service using the credentials \n",
    "        webmasters_service = build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "        maxRows = 25000\n",
    "        i = 0\n",
    "        output_rows = []\n",
    "        start_date = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        def date_range(start_date, end_date, delta=timedelta(days=1)):\n",
    "\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                yield current_date\n",
    "                current_date += delta\n",
    "        print('script start date:', start_date)\n",
    "\n",
    "        for date in date_range(start_date, end_date):\n",
    "            date = date.strftime(\"%Y-%m-%d\")\n",
    "            i = 0\n",
    "            while True:\n",
    "\n",
    "                request = {\n",
    "                    'startDate' : date,\n",
    "                    'endDate' : date,\n",
    "                    'dimensions' : [\"page\",'query'],\n",
    "                    \"searchType\": \"Web\",\n",
    "                    'rowLimit' : maxRows,\n",
    "                    'startRow' : i * maxRows\n",
    "                }\n",
    "\n",
    "                response = webmasters_service.searchanalytics().query(siteUrl = SITE_URL, body=request).execute()\n",
    "                if response is None:\n",
    "                    break\n",
    "                if 'rows' not in response:\n",
    "                    break\n",
    "                else:\n",
    "                    for row in response['rows']:\n",
    "                        page = row['keys'][0]\n",
    "                        keyword = row['keys'][1]\n",
    "                        output_row = [ page,keyword, row['clicks'], row['impressions'], row['ctr'], row['position']]\n",
    "                        output_rows.append(output_row)\n",
    "                    i = i + 1\n",
    "        print('script end date:', end_date)\n",
    "\n",
    "        df = pd.DataFrame(output_rows, columns=['Address','Keyword', 'Clicks', 'Impressions', 'CTR',  'Average Position'])\n",
    "        df = df.groupby(['Address','Main Keyword']).agg({'Clicks':'sum','Impressions':'sum','Average Position':'mean'}).reset_index()\n",
    "        df['CTR'] = df['Clicks'] / df['Impressions'] \n",
    "        return df\n",
    "   \n",
    "    def screaming_frog_crawl(self):\n",
    "        website = self.website\n",
    "        output_folder = self.output_folder\n",
    "        sf_command = os.system('cd \"C:\\Program Files (x86)\\Screaming Frog SEO Spider\" && ScreamingFrogSEOSpiderCli.exe --crawl {} --headless --output-folder {} --export-tabs \"Internal:All\"'\\\n",
    "            .format(website,output_folder))\n",
    "    \n",
    "    def keyword_check(self):\n",
    "        sf_crawl = self.screaming_frog_crawl()\n",
    "        keyword_data = self.gsc_kw()\n",
    "        output_folder = self.output_folder\n",
    "        \n",
    "        \n",
    "        # Returns top 5 keywords by clicks per URL\n",
    "        keyword_data = keyword_data.sort_values(by = 'Clicks', ascending = False).groupby(['Address']).head(5)\n",
    "        \n",
    "        df = pd.read_csv(output_folder + '\\internal_all.csv') \n",
    "        df = df[df['Indexability'] == 'Indexable'][['Address','Title 1','H1-1','Status Code','Word Count']]\n",
    "        \n",
    "        \n",
    "        df = df.merge(keyword_data, how = 'left', on = 'Address')\n",
    "\n",
    "        \n",
    "        def kw_check_percentage(df):\n",
    "            title_header = df[0]\n",
    "            keyword = df[1]\n",
    "            title_header_data = set([string for string in re.split(' ',title_header.lower().replace('|','').replace('-','').replace(r':','').replace(',','').replace(r'r\\n','').replace('&', '').replace(',', '').strip(' ')) if string != \"\"])\n",
    "            kw_data = set([string for string in re.split(' ',keyword.lower().replace('|','').replace('-','').replace(r':','').replace(',','').replace(r'r\\n','').replace('&', '').strip(' ')) if string != \"\"])\n",
    "\n",
    "            return len(title_header_data.intersection(kw_data)) /len(kw_data)\n",
    "            \n",
    "        df['KW Percentage in Title'] = df[['Title 1','Keyword']]    \n",
    "        df['KW Percentage in H1'] = df[['Title 1','H1-1']]  \n",
    "        df['KW Percentage in H2'] = df[['Title 1','H2-1']]   \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the Code Below and Audit accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_check = keyword_check(website  = 'https://yourwebsite.com',output_folder = 'your_output_folder'\n",
    "                         ,start_date = '2022-10-01',end_date = '2022-10-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kw_check.keyword_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
